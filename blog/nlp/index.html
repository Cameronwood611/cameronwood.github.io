<!doctype html><html lang=en><head><meta charset=utf-8><meta content="Official website" name=description><meta content="width=device-width,initial-scale=1.0,user-scalable=yes" name=viewport><title>Cameron Wood</title><link href=/style.css rel=stylesheet><body><article><nav class=sections><ul class=left><li><a href=/>main</a><li><a class=selected href=/blog>blog</a></ul><div class=right><a href=https://github.com/cameronwood611 target=_blank><img alt=github src=/img/github.svg></a><a href=https://gitlab.com/cameronwood611 target=_blank><img alt=gitlab src=/img/gitlab.svg></a><a href=mailto:cwood611@uab.edu><img alt=email src=/img/mail.svg></a></div></nav><main><h1 class=title>Natural Language Processing</h1><div class=time><p>2021-09-12<p>last updated 2021-12-13</div><h1 id=index>Index</h1><ul><li><a href=https://cwood.dev/blog/nlp/#background>Background</a><li><a href=https://cwood.dev/blog/nlp/#text-normalization>Text Normalization</a><li><a href=https://cwood.dev/blog/nlp/#language-models>Language Models</a><li><a href=https://cwood.dev/blog/nlp/#neural-network-language-models>Neural Network Language Models</a> <ul><li><a href=https://cwood.dev/blog/nlp/#long-short-term-memory-lstm>LSTM</a><li><a href=https://cwood.dev/blog/nlp/#gated-recurrent-unit-gru>GRU</a><li><a href=https://cwood.dev/blog/nlp/#transformer-attention-is-all-you-need>Transformer</a><li><a href=https://cwood.dev/blog/nlp/#bert-bidirectional-encoder-representations-from-transformers>BERT</a></ul></ul><h2 id=background>Background</h2><p>Natural Language Processing (NLP) is used in many fields. Some examples include Speech Recognition, Biomedical Data, Linguistics, Auto-complete, Auto-correct etc. If we break up the words contained within NLP, we have a better understanding of what it means. Natrual languages are English, Spanish, Italian, French, German and so on. Processing just means some work on some input (in this case, a computer that receives speech and text) to result in some useful output (classifier, words, speech etc.).<p><strong>Let me be clear, this field is broad and changes very quickly. I will only scratch the surface and one should reference other resources if they'd like to learn more (I'd recommend the NLTK book).</strong></p><br><br><h3 id=nlp-pipeline>NLP Pipeline</h3><p>There are a few topics that encompass the study of NLP. You may start at any one of these and work up/down the pipeline. They generally start and continue as follows:<ul><li>Phonetics: Study of Human speech/sound.<li>Phonology: Mental processing of sounds.<li>Morphology: Internal structure of words.<li>Syntax: Rules for how words are put together in a language.<li>Semantics: Meaning behind word(s).<li>Pragmatics: Understanding text in its entire context (who/what/where/when).</ul><h3 id=common-terms>Common Terms</h3><p>Here are some explained terms that you might come accross:<ul><li><strong>Corpus</strong> (plural corpora) - Collection of documents.<li><strong>Lemma</strong> - Also known as stem, it is the root of a word (i.e. run has the same lemma as running).<li><strong>Types</strong> - Distinct elements of corpus. Also called <strong>vocab</strong>.<li><strong>Tokens</strong> - Parsed elements without any normalization.<li><strong>Affix</strong> - Part of a word that gives grammatical function; could be prefix, suffix, infix & circumfix.<li><strong>Morpheme</strong> - Smallest meaningful units of a word; containes stems and affixes.<li><strong>Stemming</strong> - Process of removing affixes crudely (am, is, are -> be).<li><strong>Lemmatization</strong> - Process of removing affixes based off the morphological analysis of a word.<li><strong>Tokenization</strong> - Separation of words using regex and/or Machine Learning classifiers.<li><strong>Segmentation</strong> - Normally, this is the separation of sentences in a corpus.<li><strong>Language Model</strong> - Historically, it is a probability distribution of a sequence of words. Today, the more mathematically advanced language models are called Nerual Language models.</ul><h2 id=text-normalization>Text Normalization</h2><p>The idea behind text normalization is to put words/tokens into a standard format.<p>Two methods for normalizing might be <strong>lemmatization</strong> and <strong>stemming</strong>. Stemming is much faster but it crudely chops off affixes, rather than doing some morphological analysis like lemmatization.<p>An example of text normalization might be the following: " <strong>How many words are in this sentence</strong>: "They lay back on the San Fransisco grass and looked at the start and their ..."<p>To normalize, one would first break this into tokens by simply splitting on spaces. This gives a count of 15 tokens, though, one might argue that San Fransisco could make it 14. Then, we could say this has 13 types (combining "and") or 12 ("they" == "their").<h2 id=language-models>Language Models</h2><p>Language models are just a probability distribution over a sequence of words; probablity of getting those words. This is a core area of research for NLP and involves autocomplete, spelling correction, audio recognition etc.<p>Historically, language models are N-gram based but they could be Neural Network models. N-gram models have their downside. For word prediction, they only work well if the training data matches the test data. This never happens in the real world; we need robust models that generalize.<h3 id=naive-bayes>Naive Bayes</h3><p>If we want a simple model that classifies a document to a particular class, then Naive Bayes is our friend. It is extremely fast with low storage. Really beneficial for small or large datasets. Common applications are any general biinary classification: poitive/negative reviews, spam detection etc. Note: there are other types of classifiers like Machine Learning or K-Nearest Neighbor but Naive Bayes is easily understood and implemented.<h2 id=neural-network-language-models>Neural Network Language Models</h2><p>Most NLP tasks today are solved using neural networks. I'll go over a few of my favorites and explain why. Essentially, any neural network used for sequence-to-sequence, one-to-sequence or sequence-to-one tasks are going to be Recurrent Neural Networks or Feed-forward self-attention networks (Transformers).<h3 id=long-short-term-memory-lstm>Long Short-Term Memory (LSTM)</h3><p>The LSTM is a recurrent neural network that is extremely good at modeling global context whlie looking at local ones in parallel. My preferred method is to use a bidirectional LSTM in order to model the future to past and past to future contexts better. I demonstrate this with my spam detection project and can be found <a href=https://github.com/Cameronwood611/nlp662-spamdetection target=_>here</a>.<p>The important components of the LSTM are that it has 4 main "gates" (mathematical functions to let information pass, or not): forget, output, input and update. Long and short-term memory cells feed into these gates (as well as normal input) and having these helps model sequence context really well.<h3 id=gated-recurrent-unit-gru>Gated-Recurrent Unit (GRU)</h3><p>There are many variations of the LSTM, and GRU is the most popular one. It combines the target and input gates into a single update gate. It also merges the cell state with the hidden state.<h3 id=transformer-attention-is-all-you-need>Transformer (Attention is all you need!)</h3><p>This model is one that shifted NLP in a positive direction. The Transformer uses something called multi-headed attention instead of recurrence. The high level overview of attention is that there is a Key, Query and Value mechanism which serves as a replacement for recurrence. This model performs the best for sequence-to-sequence and translation tasks (huggingface is a great library for this).<p>It also includes two mechanisms within the heads: an encoder that reads the text input and a decoder that produces a prediction for the task. It learns contextual relations between words really well.<h3 id=bert-bidirectional-encoder-representations-from-transformers>BERT (Bidirectional Encoder Representations from Transformers)</h3><p>BERT is a variation of the Transformer that drops the decoder mechanism and applies a "masking" layer. This requires the neural network to predict the original value based on the context when it is masked. BERT does really well with tasks such as Question and Answering.</main><footer><div><p>Curiosity and drive makes a good programmer.</div></footer></article>