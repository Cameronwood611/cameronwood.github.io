<!DOCTYPE html><html lang=en><head><meta charset=utf-8><meta name=description content="Official website"><meta name=viewport content="width=device-width, initial-scale=1.0, user-scalable=yes"><title>Cameron Wood</title><link rel=stylesheet href=/style.css><body><article><nav class=sections><ul class=left><li><a href=/>main</a><li><a href=/blog class=selected>blog</a></ul><div class=right><a href=https://github.com/cameronwood611 target=_blank><img src=/img/github.svg alt=github></a><a href=https://gitlab.com/cameronwood611 target=_blank><img src=/img/gitlab.svg alt=gitlab></a><a href=mailto:cwood611@uab.edu><img src=/img/mail.svg alt=email></a></div></nav><main><h1 class=title>NLP</h1><div class=time><p>2021-09-12<p>last updated 2021-09-13</div><h1 id=index>Index</h1><ul><li><a href=https://cameronwood611.github.io/blog/nlp/#background>Background</a><li><a href=https://cameronwood611.github.io/blog/nlp/#text-normalization>Text Normalization</a></ul><h2 id=background>Background</h2><p>Natural Language Processing (NLP) is used in many fields. Some examples include Speech Recognition, Biomedical Data, Linguistics, Auto-complete, Auto-correct etc. If we break up the words contained within NLP, we have a better understanding of what it means. Natrual languages are English, Spanish, Italian, French, German and so on. Processing just means some work on some input (in this case, a computer that receivees speech and text) to result in some useful output (classifier, words, speech etc.).<h3 id=nlp-pipeline>NLP Pipeline</h3><p>There are a few topics that encompass the study of NLP. You may start at any one of these and work up/down the pipeline. They generally start and continue as follows:<ul><li>Phonetics: Study of Human speech/sound.<li>Phonology: Mental processing of sounds.<li>Morphology: Internal structure of words.<li>Syntax: Rules for how words are put together in a language.<li>Semantics: Meaning behind word(s).<li>Pragmatics: Understanding text in its entire context (who/what/where/when).</ul><h3 id=common-terms>Common Terms</h3><p>Here are some explained terms that you might come accross:<ul><li><strong>Corpus</strong> (plural corpora) - Collection of documents.<li><strong>Lemma</strong> - Also known as stem, it is the root of a word (i.e. run has the same lemma as running).<li><strong>Types</strong> - Distinct elements of corpus. Also called <strong>vocab</strong>.<li><strong>Tokens</strong> - Parsed elements without any normalization.<li><strong>Affix</strong> - Part of a word that gives grammatical function; could be prefix, suffix, infix & circumfix.<li><strong>Morpheme</strong> - Smallest meaningful units of a word; containes stems and affixes.<li><strong>Stemming</strong> - Process of removing affixes crudely (am, is, are -> be).<li><strong>Lemmatization</strong> - Process of removing affixes based off the morphological analysis of a word.<li><strong>Tokenization</strong> - Separation of words using regex and/or Machine Learning classifiers.<li><strong>Segmentation</strong> - Normally, this is the separation of sentences in a corpus.<li><strong>Language Model</strong> - Historically, it is a probability distribution of a sequence of words.</ul><h2 id=text-normalization>Text Normalization</h2><p>The idea behind text normalization is to put words/tokens into a standard format.<p>Two methods for normalizing might be <strong>lemmatization</strong> and <strong>stemming</strong>. Stemming is much faster but it crudely chops off affixes, rather than doing some morphological analysis like lemmatization.<p>An example of text normalization might be the following: " <strong>How many words are in this sentence</strong>: "They lay back on the San Fransisco grass and looked at the start and their ..."<p>To normalize, one would first break this into tokens by simply splitting on spaces. This gives a count of 15 tokens, though, one might argue that San Fransisco could make it 14. Then, we could say this has 13 types (combining "and") or 12 ("they" == "their").<h2 id=language-models>Language Models</h2><p>Language models are just a probability distribution over a sequence of words; probablity of getting those words. This is a core area of research for NLP and involves autocomplete, spelling correction, audio recognition etc.<p>Historically, language models are N-gram based but they could be Neural Network models. N-gram models have their downside. For word prediction, they only work well if the training data matches the test data. This never happens in the real world; we need robust models that generalize.<h3 id=naive-bayes>Naive Bayes</h3><p>If we want a simple model that classifies a document to a particular class, then Naive Bayes is our friend. It is extremely fast with low storage. Really beneficial for small or large datasets. Common applications are any general biinary classification: poitive/negative reviews, spam detection etc. Note: there are other types of classifiers like Machine Learning or K-Nearest Neighbor but Naive Bayes is easily understood and implemented.</main><footer><div><p>Built with the help of <a href=https://www.getzola.org target=_blank>Zola</a></div></footer></article>