<!doctype html><html lang=en><head><meta charset=utf-8><meta content="Official website" name=description><meta content="width=device-width,initial-scale=1.0,user-scalable=yes" name=viewport><title>Cameron Wood</title><link href=/style.css rel=stylesheet><body><article><nav class=sections><ul class=left><li><a href=/>main</a><li><a class=selected href=/blog>blog</a></ul><div class=right><a href=https://github.com/cameronwood611 target=_blank><img alt=github src=/img/github.svg></a><a href=https://gitlab.com/cameronwood611 target=_blank><img alt=gitlab src=/img/gitlab.svg></a><a href=mailto:cwood611@uab.edu><img alt=email src=/img/mail.svg></a></div></nav><main><h1 class=title>Computer Graphics (Matrix Algorithms)</h1><div class=time><p>2022-01-02<p>last updated 2022-01-04</div><h1 id=index>Index</h1><ul><li><a href=http://cwood.dev/blog/matrix-algs/#background>Background</a><li><a href=http://cwood.dev/blog/matrix-algs/#householder-transformation>Householder Transformation</a><li><a href=http://cwood.dev/blog/matrix-algs/#qr-decomposition>QR Decomposition</a><li><a href=http://cwood.dev/blog/matrix-algs/#>Principal Component Analysis</a><li><a href=http://cwood.dev/blog/matrix-algs/#>SVD</a><li><a href=http://cwood.dev/blog/matrix-algs/#>Subdivision Curves</a><li><a href=http://cwood.dev/blog/matrix-algs/#>Bezier Curves</a><li><a href=http://cwood.dev/blog/matrix-algs/#>Cubic B-splines</a></ul><h2 id=background>Background</h2><p>I took and TA'd for a course called Matrix Algorithms which uses advanced linear algebra techniques for numerical computing and computer graphics. Understanding this material is nearly impossible if you don't have prior experience with linear algebra. I'll be metioning the few topics I found useful and their applications.<h2 id=householder-transformation>Householder Transformation</h2><p>Orthogonal matrices have some nice properties: they rotate and reflect. As it rotates, the distance is preserved. A Householder matrix is orthogonal and its main goal is to reflect, or "mirror", a vector across a hyperplane.<p>You have some hyperplane with a normal <em>u</em> (assume unit vector) and the vector <em>x</em> that you are trying to reflect. Togeher, the span of <em>u</em> and <em>x</em> form a two dimensional space. You take the dot product of the two vectors and multiply it by <em>u</em> (the component of <em>x</em> in the <em>u</em> direction): (<strong>u</strong><sup>t</sup><strong>x</strong>)<strong>u</strong> . You change the direction and double this to reach your desired reflected vector: <em>x</em> - 2(<em>u</em><sup> t</sup><em>x</em>)<em>u</em> . Currently, this is just an expression and the matrix that does this transformation (Householder) is as follows: (I - 2 <em>u</em> <em>u</em><sup> t </sup>). The householder vector is <em>u</em>, the normal of the reflecting hyperplane.<p>Many algorithms will use the Householder transformation matrix, mosty when premultiplying existing matrices. The time complexity of Householder is <strong>O(n<sup>2</sup>)</strong> while its space is <strong>O(n)</strong> (we normalize <em>u</em>).<h2 id=qr-decomposition>QR Decomposition</h2><p>QR Decomposition involves rectangular, orthogonal and upper triangular matrices. With LU decomposition, we could only work with square matrices. Now, we're able to work with more general things, like least squares, solving linear systems, finding orthogonal bases (Gram-Schmidt) and spectral analysis.<p>In general, QR is good for building orthogonal matrices but, much of it is used to push towards upper triangular matrices for effeciency.<p>Q - Orthogonal, R - Upper Triangular</main><footer><div><p>Curiosity and drive makes a good programmer.</div></footer></article>